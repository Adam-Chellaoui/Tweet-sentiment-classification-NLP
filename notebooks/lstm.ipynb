{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [I. Putting data into fastText embedding](#chapter1)\n",
    "   * [1. Loading the FastText model to embedd the data](#section_1_1)\n",
    "   * [2. Loading the tweets](#section_1_2)\n",
    "   * [3. Displaying the distribution of tweet's length (usefull for the padding)](#section_1_3)\n",
    "   * [4. Embedding the tweets with FastText](#section_1_4)\n",
    "   * [5. Pad the embedded tweets](#section_1_5)\n",
    "   * [6. Split into training and validating set](#section_1_6)\n",
    "* [II. Building the LSTM model](#chapter2)\n",
    "    * [1. Architecture of the LSTM](#section_2_1)\n",
    "    * [1. Training of the LSTM](#section_2_2)\n",
    "* [III. Making the predictions for Submission](#chapter3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '/Users/douglasbouchet/M1/ML/ml_project_2_sentwiment/src')\n",
    "\n",
    "import sklearn\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.losses import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras.activations import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from load_utils import load_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Putting data into fastText embedding <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the fastText model to embedd data  <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# We assume that you have followed the step of the readme, and that you have put the model at the correct place\n",
    "model = fasttext.load_model(\"../data/word_Embeddings/FastText/fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading the tweets  <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our small dataframe (the training of lstm requires to much time if trained on the full data, and does not provide significant increase of accuracy)\n",
    "df = load_df(full=False,lemmatize=True)\n",
    "\n",
    "y = df.positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Displaying the distribution of tweet's length (usefull for the padding)  <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each tweet, we record its number of words\n",
    "tweet_len = {}\n",
    "for elem in df.tweet: \n",
    "    l = len(elem.split(\" \"))\n",
    "    old_val = tweet_len.get(l) or 0\n",
    "    tweet_len[l] = old_val + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_len = collections.OrderedDict(sorted(tweet_len.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "plt.xlabel(\"Number of words per tweet\")\n",
    "plt.ylabel(\"Number of tweet\")\n",
    "# There are almost no tweet longer than 30 words, so we avoid plotting them for a better visualization\n",
    "plt.bar(range(len(tweet_len))[0:30], list(tweet_len.values())[0:30], align='center')\n",
    "plt.xticks(range(len(tweet_len))[0:30], list(tweet_len.keys())[0:30])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Embedding the tweet with fastText  <a class=\"anchor\" id=\"section_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/f5sm00lj343gkrnlrbwvbls00000gn/T/ipykernel_20007/1244914488.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  embedded = np.asarray(embedded)\n"
     ]
    }
   ],
   "source": [
    "# For each word of each tweet, we compute its fastText embedding \n",
    "embedded = []\n",
    "for tweet in df.tweet:\n",
    "    vec = []\n",
    "    for word in tweet.split(\" \"):\n",
    "        # get the embedding of the word\n",
    "        vec.append(model.get_word_vector(word))\n",
    "    embedded.append(np.asarray(vec))\n",
    "embedded = np.asarray(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pad the embedded tweets  <a class=\"anchor\" id=\"section_1_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need all our inputs of our LSTM to have same length, that's why we apply padding. 22 is the optimal size (explained in the report). 'pre' is also better\n",
    "# than 'post'. Using a padding of 22 allows to capture all features of almost all tweets (see figure above).\n",
    "padded_x = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    embedded, maxlen=22, padding=\"pre\", dtype='float32'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Splitting into training and validating set  <a class=\"anchor\" id=\"section_1_6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196664, 22, 300), (49166, 22, 300), (196664,), (49166,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a valid set of 20% of the total data\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_x, y, test_size=0.2, random_state=26105111)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the LSTM will have a sigmoid as activation function, that's why we transform the -1 labels into 0\n",
    "new_y_train = []\n",
    "for i,elem in enumerate(y_train):\n",
    "    if(elem == -1):\n",
    "        new_y_train.append(0)\n",
    "    else: \n",
    "        new_y_train.append(1)\n",
    "\n",
    "new_y_test = []\n",
    "for i,elem in enumerate(y_test):\n",
    "    if(elem == -1):\n",
    "        new_y_test.append(0)\n",
    "    else: \n",
    "        new_y_test.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Building the LSTM model <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Architecture of the LSTM  <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 22, 32)            28832     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 22, 32)            3104      \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 22, 32)            3104      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 7, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 1024)             2232320   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,317,985\n",
      "Trainable params: 3,317,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douglasbouchet/anaconda3/envs/ml/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "# We declare the architecture of the LSTM.\n",
    "# To resume what we said in the report:\n",
    "#  Conv1D capture topics\n",
    "#  MaxPadding capture most important feature\n",
    "#  LSTM allow take into account the order of words inside the tweet\n",
    "#  Dropout help reduce the overfitting\n",
    "#  Binary classification so 1 output neuron with sigmoid activation function\n",
    "\n",
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',\n",
    "                 input_shape=(22, 300)))\n",
    "lstm.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "lstm.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
    "lstm.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "lstm.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
    "\n",
    "lstm.add(Dense(512, activation='sigmoid'))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(Dense(512, activation='sigmoid'))\n",
    "lstm.add(Dropout(0.25))\n",
    "lstm.add(Dense(512, activation='sigmoid'))\n",
    "lstm.add(Dropout(0.25))\n",
    "\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# We used the adam optimizer. Learning rate has been tuned (see report). Use binary_crossentropy and accuracy to correctly measure performances of our network \n",
    "lstm.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training of the model  <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "385/385 [==============================] - 354s 901ms/step - loss: 0.4672 - accuracy: 0.7584 - val_loss: 0.3854 - val_accuracy: 0.8184\n",
      "Epoch 2/5\n",
      "385/385 [==============================] - 307s 796ms/step - loss: 0.3847 - accuracy: 0.8206 - val_loss: 0.3739 - val_accuracy: 0.8266\n",
      "Epoch 3/5\n",
      "385/385 [==============================] - 350s 910ms/step - loss: 0.3692 - accuracy: 0.8288 - val_loss: 0.3641 - val_accuracy: 0.8316\n",
      "Epoch 4/5\n",
      "385/385 [==============================] - 338s 878ms/step - loss: 0.3572 - accuracy: 0.8352 - val_loss: 0.3594 - val_accuracy: 0.8343\n",
      "Epoch 5/5\n",
      "385/385 [==============================] - 334s 867ms/step - loss: 0.3508 - accuracy: 0.8392 - val_loss: 0.3614 - val_accuracy: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faaa0425700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we fit LSTM on our training data. Batch size and epochs have been tuned (see report).\n",
    "# The expected runing time should be ~ TODO (without GPU)\n",
    "lstm.fit(x_train, pd.Series(new_y_train), batch_size=512, shuffle=True, epochs=5,\n",
    "         validation_data=(x_test, pd.Series(new_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making the predictions for Submission <a class=\"anchor\" id=\"chapter_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/f5sm00lj343gkrnlrbwvbls00000gn/T/ipykernel_20007/2783233971.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  embedded = np.asarray(embedded)\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv('../data/data_submission_preprocessed.csv')\n",
    "\n",
    "# For each word of each tweet, we compute its fastText embedding \n",
    "embedded = []\n",
    "for tweet in data_test.tweet:\n",
    "    vec = []\n",
    "    for word in tweet.split(\" \"):\n",
    "        # get the embedding of the word\n",
    "        vec.append(model.get_word_vector(word))\n",
    "    embedded.append(np.asarray(vec))\n",
    "embedded = np.asarray(embedded)\n",
    "\n",
    "# pad the inputs\n",
    "padded_x = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    embedded, maxlen=22, padding=\"pre\", dtype='float32'\n",
    ")\n",
    "\n",
    "# for each tweet, we use the LSTM to predict a label\n",
    "predictions = lstm(padded_x)\n",
    "\n",
    "# we replace labels by -1 if it was less than 0.5 else by 1, as we use a sigmoid at the end of the LSTM\n",
    "res = []\n",
    "for i,elem in enumerate(predictions):\n",
    "    if(elem >= 0.5):\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(-1)\n",
    "#We rename the Id column in order to be accepted in the submission platform\n",
    "data_test = data_test.rename(columns={'tweet_idx':'Id'})\n",
    "# We add the prections column\n",
    "data_test['Prediction'] = res\n",
    "# We save to a csv file only Id, Prediction columns\n",
    "data_test[['Id', 'Prediction']].to_csv('../submission.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4bf98731e39b0f330e4e7fe339590b91375d5c2f965cb189d278f0fc33c4d2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
